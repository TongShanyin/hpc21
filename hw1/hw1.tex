\documentclass[10pt,a4paper]{article}
\usepackage[centertags]{amsmath}
\usepackage{amsfonts,amssymb, amsthm}
\usepackage{hyperref}
\usepackage{comment}
\usepackage[shortlabels]{enumitem}
\usepackage{bm}

\usepackage{cite,graphicx,color}
%\usepackage{fourier}
\usepackage[margin=1.5in]{geometry}
\usepackage{enumitem}
\usepackage{bbm}

\usepackage{tikz,pgfplots}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

\usepackage{mathtools}
%\mathtoolsset{showonlyrefs} % only show no. of refered eqs

\usepackage{cleveref}

\textheight 8.5in

\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}

\newtheoremstyle{dotlessP}{}{}{\color{blue!50!black}}{}{\color{blue}\bfseries}{}{ }{}
\theoremstyle{dotlessP}
\newtheorem{question}{Question}



\def\VV{\mathbb{V}}
\def\EE{\mathbb{E}}
\def\PP{\mathbb{P}}
\def\RR{\mathbb{R}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\mF}{F}%{\mathcal{F}}

\DeclareMathOperator{\sgn}{sgn}
%\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\erfc}{erfc}
\DeclareRobustCommand{\argmin}{\operatorname*{argmin}}
\DeclareRobustCommand{\arginf}{\operatorname*{arginf}}

\def\EE{\mathbb{E}}\def\PP{\mathbb{P}}
\def\NN{\mathbb{N}}\def\RR{\mathbb{R}}\def\ZZ{\mathbb{Z}}



\def\<{\left\langle} \def\>{\right\rangle}







\newcommand{\emphasis}[1]{\textcolor{red!80!black}{#1}}
\newcommand{\shanyin}[1]{\textcolor{blue!80!black}{#1}}

% ****************************
\begin{document}


\title{High performance computing HW1}
\author{Shanyin Tong, st3255@nyu.edu}

\maketitle

\section{Describe a parallel application and the algorithms used}
The parallel application problem is the earthquake hazard assessments, where researchers want to create an end-to-end, fault-to-structure simulation framework of the ground motions and building response after the occurrence of slips at the faults. A heroic simulation of an earthquake rupture that may take 30 – 40 h or more on today’s fastest scientific computers, the possibility of routinely simulating high-frequency regional-scale earthquake scenarios of the order of 3–5 h is within grasp on exascale platforms. To simulate the whole process, a computationally optimized, high-order of accuracy, explicit finite-difference code for simulating ground motions using kinematic earthquake rupture models, and the subsequent coupling of the geophysics simulations to implicit, nonlinear finite element representations of infrastructure system response. SW4 (Seismic Waves, 4th order) a Summation-By-Parts (SBP) finite-difference program for earthquake ground motion simulation, which solves the viscoelastic wave equation with fourth-order accuracy in both space and time. SW4 geophysics model grid are distributed across thousands of nodes on a parallel computer platform by subdividing the overall model domain into vertical pencil-shaped subdomains which extend from the earth surface to the bottom of the computational domain. Each pencil is distributed to separate machine nodes using a parallel load-balancing algorithm that decomposes the computational tasks that are distributed across the parallel computer platform. The associated message passing interface (MPI) implementation must be optimized for the particular node and core architecture associated with any specific computer platform. The researchers use both supercomputers: Cori and Summit (No.2 in the Top 500 list). Compared with the initial run, the paralleled codes take about one third of the running times.

Reference: McCallen, David, Anders Petersson, Arthur Rodgers, Arben Pitarka, Mamun Miah, Floriana Petrone, Bjorn Sjogreen, Norman Abrahamson, and Houjun Tang. 2020. “EQSIM—A Multidisciplinary Framework for Fault-to-Structure Earthquake Simulations on Exascale Computers Part I: Computational Models and Workflow.” Earthquake Spectra, December, 8755293020970982. https://doi.org/10.1177/8755293020970982.

\section{Matrix-matrix multiplication}
The processor I use for timings is "AMD Opteron(TM) Processor 6272", and the complier version is "gcc version 4.8.5 20150623 (Red Hat 4.8.5-44) (GCC) ". As the problem suggested, $C=C+AB$, where $C\in \RR^{m\times n}, A\in \RR^{m\times k}, B \in\RR^{k \times n}$. The total flops for one matrix-matrix multiplication is $\sim 2mnk$ and the memory read/write is $\sim 3mn+mnk$. In the program, we set $m=n=p$. For different optimization flags, I obtain the results as shown in \cref{tab:mm0,tab:mm3}.

	\begin{table}[tbhp] 
	{\footnotesize
		\caption{Results for matrix-matrix multiplication with optimization flags -O0
		}\label{tab:mm0}
		\begin{center}
			\begin{tabular}{cccc}
				\hline 
				Dimension  $p$ &    Time [s] &  Flop rate [Gflop/s]     & Bandwidth  [GB/s] \\ 
				\hline 
        20 &   0.007130 &    0.112202 &   0.516129 \\ 
40 &   0.057982 &    0.110378 &   0.474627 \\ 
60 &   0.196542 &    0.109900 &   0.461580 \\ 
80 &   0.451118 &    0.113496 &   0.471008 \\ 
100 &   0.908824 &    0.110032 &   0.453333 \\ 
120 &   1.534083 &    0.112641 &   0.461826 \\ 
140 &   2.627633 &    0.104429 &   0.426665 \\ 
160 &   4.136050 &    0.099032 &   0.403554 \\ 
180 &   5.503290 &    0.105973 &   0.430957 \\ 
200 &   7.526170 &    0.106296 &   0.431561 \\ 
220 &  10.398512 &    0.102399 &   0.415182 \\ 
240 &  15.245309 &    0.090677 &   0.367242 \\ 
260 &  18.006862 &    0.097607 &   0.394934 \\ 
280 &  22.864299 &    0.096010 &   0.388154 \\ 
300 &  29.175589 &    0.092543 &   0.373874 \\ 
320 &  34.873556 &    0.093962 &   0.379373 \\ 
340 &  42.454289 &    0.092580 &   0.373586 \\ 
360 &  51.251318 &    0.091034 &   0.367169 \\ 
380 &  64.430304 &    0.085165 &   0.343349 \\ 
400 &  70.419664 &    0.090884 &   0.366261 \\ 
420 &  86.552622 &    0.085599 &   0.344841 \\ 
440 &  95.732863 &    0.088981 &   0.358351 \\ 
460 & 114.431708 &    0.085060 &   0.342460 \\ 
480 & 123.769261 &    0.089353 &   0.359647 \\ 
500 & 146.984738 &    0.085043 &   0.342212 \\ 
520 & 161.326708 &    0.087157 &   0.350641 \\ 
540 & 185.117903 &    0.085061 &   0.342136 \\ 
560 & 199.154003 &    0.088181 &   0.354614 \\ 
580 & 229.523469 &    0.085007 &   0.341788 \\
				\hline 
			\end{tabular} 
		\end{center}
	}
\end{table}

	\begin{table}[tbhp] 
	{\footnotesize
		\caption{Results for matrix-matrix multiplication with optimization flags -O3
		}\label{tab:mm3}
		\begin{center}
			\begin{tabular}{cccc}
				\hline 
 Dimension  $p$ &    Time [s] &  Flop rate [Gflop/s]     & Bandwidth  [GB/s] \\ 
 \hline 
20 &   0.001294 &    0.618137 &   2.843429 \\ 
40 &   0.008549 &    0.748604 &   3.218995 \\ 
60 &   0.027563 &    0.783650 &   3.291332 \\ 
80 &   0.065075 &    0.786784 &   3.265152 \\ 
100 &   0.121443 &    0.823434 &   3.392547 \\ 
120 &   0.207566 &    0.832506 &   3.413275 \\ 
140 &   0.381197 &    0.719838 &   2.941051 \\ 
160 &   0.736348 &    0.556259 &   2.266754 \\ 
180 &   0.856420 &    0.680974 &   2.769294 \\ 
200 &   1.197803 &    0.667890 &   2.711632 \\ 
220 &   1.685210 &    0.631850 &   2.561865 \\ 
240 &   2.536818 &    0.544935 &   2.206985 \\ 
260 &   2.848004 &    0.617134 &   2.497019 \\ 
280 &   3.752274 &    0.585032 &   2.365200 \\ 
300 &   4.731781 &    0.570610 &   2.305263 \\ 
320 &   6.017875 &    0.544511 &   2.198464 \\ 
340 &   7.096058 &    0.553885 &   2.235089 \\ 
360 &   8.674678 &    0.537841 &   2.169293 \\ 
380 &  10.354615 &    0.529928 &   2.136446 \\ 
400 &  12.252473 &    0.522344 &   2.105044 \\ 
420 &  14.585056 &    0.507972 &   2.046402 \\ 
440 &  17.245586 &    0.493947 &   1.989258 \\ 
460 &  20.280078 &    0.479959 &   1.932355 \\ 
480 &  23.757322 &    0.465507 &   1.873666 \\ 
500 &  27.757283 &    0.450332 &   1.812137 \\ 
520 &  32.035897 &    0.438908 &   1.765759 \\ 
540 &  36.078566 &    0.436448 &   1.755489 \\ 
560 &  40.251775 &    0.436294 &   1.754524 \\ 
580 &  44.844218 &    0.435088 &   1.749356 \\
				\hline 
			\end{tabular} 
		\end{center}
	}
\end{table}

In \cref{tab:mm0,tab:mm3}, we observe the flop rate and bandwidth of -O3 is about six times of those of -O3. As dimension increases, the flop rate and bandwidth decrease, but their ratio remains the same, which is consistent with the bound from the computational intensity.

The codes are 
\lstinputlisting[language=C++]{MMult0.cpp}
\section{Laplace equation}
I use the same computer architecture as in the last section. For either Jacobi of Gauss-Seidel method, the solutions do not converge in 5000 iterations, so I also report the norm of residual to compare the decay speed of residual, i.e., $\|Au_n-f\|/\|Au_0-f\|$, where $n$ is the final iteration. First, I implement a dense version for Jacobi and Gauss-Seidel methods for general linear system $Au=f$, but it runs too slow, as shown in \cref{tab:Laplace-dense}. Then, I implement a sparse version of the algorithms work for the specific Laplace equation we solve, then the computing times decrease, the corresponding results are shown in \cref{tab:Laplace-sparse}.
	\begin{table}[tbhp] 
	{\footnotesize
		\caption{Comparison of Jacobi and Gauss-Seidel methods for solving Laplace equation for $N=100$ using dense implementation (with compiler optimization flag -O3), note the initial residual is 10 for $N=100$.
		}\label{tab:Laplace-dense}
		\begin{center}
\begin{tabular}{|c|c|c|c|c|}
	\hline 
Method	& Iterations & Final residual & Residual decay factor &  Time[s]\\ 
	\hline 
Jacobi& 5000 & 0.805169  & 12.419754 & 0.598524  \\ 
%	\hline 
Gauss-Seidel	&  5000  & 0.071734  & 139.403106 & 0.333994 \\ 
	\hline 
\end{tabular} 
		\end{center}
	}
\end{table}

	\begin{table}[tbhp] 
	{\footnotesize
		\caption{Comparison of Jacobi and Gauss-Seidel methods for solving Laplace equation for $N=100$ and $N=10000$ using sparse implementation for Laplace equation (with compiler optimization flag -O3), note the initial residual is 10 for $N=100$ and the initial residual is 100 for $N=10000$. 
		}\label{tab:Laplace-sparse}
		\begin{center}
			\begin{tabular}{|c|c|c|c|c|}
				\hline 
				Method	& Iterations & Final residual & Residual decay factor &  Time[s]\\ 
				\hline 
				Jacobi($N=100$)	& 5000 & 0.805169  & 12.419754 &  0.078667  \\ 
				%	\hline 
				Gauss-Seidel($N=100$)	&  5000  & 0.071734  & 139.403106 & 0.097494 \\ 
				\hline 
			Jacobi($N=10000$)		& 5000 &   99.203927 & 1.008025 & 0.686890 \\ 
				%	\hline 
			Gauss-Seidel($N=10000$)	& 5000  & 98.870274  & 1.011426 & 2.200062 \\ 
				\hline 
			\end{tabular} 
		\end{center}
	}
\end{table}

In \cref{tab:Laplace-sparse}, we can see that the residual of Gauss-Seidel decays faster than Jacobi method, even obtain one more magnitude decay for $N=100$ . As for finer mesh $N=10000$, their residual both decay small. The slow convergence of the residual is because for each iteration, the element only interact with its neighboring elements, thus do not have enough update, while Gauss-Seidel uses newly computed left neighbor, which provide a little more information, thus obtain faster convergence. 


	\begin{table}[tbhp] 
	{\footnotesize
		\caption{Comparison of run times of 100 iterations for Jacobi and Gauss-Seidel methods for solving Laplace equation for $N=10000$ using sparse implementation for Laplace equation, with different compiler optimization flags -O0 and -O3.
		}\label{tab:Laplace-time}
		\begin{center}
			\begin{tabular}{|c|c|c|}
				\hline 
				Method	& Compiler optimization flag & Run time[s]\\ 
				\hline 
  Jacobi & -O0 & 0.083938\\
 Jacobi & -O3 &  0.013610\\
  \hline
  Gauss-Seidel &-O0 & 0.101125\\
  Gauss-Seidel &-O3 & 0.044627\\
				\hline 
			\end{tabular} 
		\end{center}
	}
\end{table}

In \cref{tab:Laplace-time}, we compare the time of two methods for different compiler optimization flags -O0 and -O3, the compiler optimization flag -O3 uses less time than that of -O0.
\newpage
The dense implementation codes are
\lstinputlisting[language=C++]{Laplace.cpp}

The sparse implementation codes are
\lstinputlisting[language=C++]{Laplace2.cpp}
\end{document}