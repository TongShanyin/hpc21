\documentclass[10pt,a4paper]{article}
\usepackage[centertags]{amsmath}
\usepackage{amsfonts,amssymb, amsthm}
\usepackage{hyperref}
\usepackage{comment}
\usepackage[shortlabels]{enumitem}
\usepackage{bm}

\usepackage{cite,graphicx,color}
%\usepackage{fourier}
\usepackage[margin=1.5in]{geometry}
\usepackage{enumitem}
\usepackage{bbm}

\usepackage{tikz,pgfplots}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\lstset{style=mystyle}

\usepackage{mathtools}
%\mathtoolsset{showonlyrefs} % only show no. of refered eqs

\usepackage{cleveref}

\textheight 8.5in

\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}
\newtheorem{proposition}{Proposition}

\newtheoremstyle{dotlessP}{}{}{\color{blue!50!black}}{}{\color{blue}\bfseries}{}{ }{}
\theoremstyle{dotlessP}
\newtheorem{question}{Question}



\def\VV{\mathbb{V}}
\def\EE{\mathbb{E}}
\def\PP{\mathbb{P}}
\def\RR{\mathbb{R}}
\newcommand{\mD}{\mathcal{D}}
\newcommand{\mF}{F}%{\mathcal{F}}

\DeclareMathOperator{\sgn}{sgn}
%\DeclareMathOperator{\erf}{erf}
\DeclareMathOperator{\erfc}{erfc}
\DeclareRobustCommand{\argmin}{\operatorname*{argmin}}
\DeclareRobustCommand{\arginf}{\operatorname*{arginf}}

\def\EE{\mathbb{E}}\def\PP{\mathbb{P}}
\def\NN{\mathbb{N}}\def\RR{\mathbb{R}}\def\ZZ{\mathbb{Z}}



\def\<{\left\langle} \def\>{\right\rangle}







\newcommand{\emphasis}[1]{\textcolor{red!80!black}{#1}}
\newcommand{\shanyin}[1]{\textcolor{blue!80!black}{#1}}

% ****************************
\begin{document}


\title{High performance computing HW3}
\author{Shanyin Tong, st3255@nyu.edu}

\maketitle

\section{OpenMP warm-up}
\begin{enumerate}[(a)]
	\item Since there are two chunks,  OpenMP divides iterations into chunks that are approximately equal in size, then each chunks is about size $n/2$. 
	
	For the first for-loop,  the execution for the first part is $$1+2+\ldots+\frac{n}{2}=\frac{1}{2}(1+\frac{n}{2})\frac{n}{2}=\frac{n(n+2)}{8},$$ the second part is 
	$$ (\frac{n}{2}+1) + \ldots + (n-1) = \frac{1}{2}( \frac{n}{2}+1+n-1)(\frac{n}{2}-1)=\frac{3n(n-2)}{8}.$$ Assume $n$ is large, the total execution time is 
	$$\max\{\frac{n(n+2)}{8}, \frac{3n(n-2)}{8}\}=  \frac{3n(n-2)}{8},$$ the wait time is $$\frac{3n(n-2)}{8} - \frac{n(n+2)}{8} = \frac{n^2-2n}{4} .$$
	
	For the second for-loop, the execution for the first part is $$(n-1)+(n-2)+\ldots+(n-\frac{n}{2})=\frac{1}{2}(n-1+n-\frac{n}{2})\frac{n}{2}=\frac{n(3n-2)}{8},$$ the second part is 
	$$ (n-\frac{n}{2}-1 )+ \ldots +(n- n+1) = \frac{1}{2}( n-\frac{n}{2}-1+n-n+1)(\frac{n}{2}-1)=\frac{n(n-2)}{8}.$$ Assume $n$ is large, the total execution time is 
	$$\max\{\frac{n(3n-2)}{8}, \frac{n(n-2)}{8}\}=  \frac{n(3n-2)}{8},$$ the wait time is $$\frac{n(3n-2)}{8} - \frac{n(n-2)}{8} = \frac{n^2}{4} .$$
	
	Thus, the total time each thread spend to execute the parallel region is $\frac{3n(n-2)}{8}+\frac{n(3n-2)}{8}=\frac{3n^2-4n}{4}$ milliseconds, the total wait time is $\frac{n^2-2n}{4}+\frac{n^2}{4}=\frac{n^2-n}{2}$ milliseconds.
	
	
	\item Using \texttt{schedule(static, 1)} means the chunk size is 1. The first thread execute iterations $1, 3, \ldots, n-1$, and the second thread execute iterations $2, 4, \ldots, n-2$. So for the first loop, thread 1 takes $1+3+\ldots+n-1=\frac{n^2}{4}$ milliseconds, thread 2 takes $\frac{n(n-2)}{4}$ milliseconds, so the total execution time is $\frac{n^2}{4}$ milliseconds, wait time is $\frac{n}{2}$ milliseconds. For the second loop, the situation is the same as the first loop. So the total execution time for each thread is $\frac{n^2}{2}$ milliseconds, smaller than (a). 
	
	\item By using \texttt{schedule(dynamic, 1)}, since OpenMP dynamically distributes the iterations during the runtime, when thread 1 takes iteration 1, thread 2 take iteration 2, but thread 1 execution time is shorter than thread 2, so thread 1 will take iteration 3... So thread 1 will take iterations $1, 3, \ldots, n-1$ and thread 2 will take iterations $2, 4, \ldots, n-2$, which is same as (b), so it will not improve the run time.
	
	\item We can use \texttt{nowait} to eliminate the wait time. When using \texttt{nowait}, in case (a), thread 1 uses $\frac{n(n+2)}{8}+\frac{n(3n-2)}{8}=\frac{n^2}{2}$ milliseconds, thread 2 uses $\frac{3n(n-2)}{8}+\frac{n(n-2)}{8}=\frac{n^2-2n}{2}$ milliseconds. In case (b), thread 1 uses $\frac{n^2}{2}$, thread 2 uses $\frac{n^2-2n}{2}$ milliseconds. In case(c), both thread uses $\frac{n^2-n}{2}$ milliseconds.
	
\end{enumerate}

\section{Finding OpenMP bugs}
The details are in the codes, for \texttt{./omp-solved4} , please do \texttt{limit stacksize unlimited} in command line first to enlarge the stacksize.

\section{Parallel Scan in OpenMP}
I use \texttt{crunchy1} to run the codes, the model name is AMD Opteron(TM) Processor 6272, and have 64 CPUs. The results are shown in \cref{tab:scan}. We denote the number of threads to be $p$, and use \texttt{setenv OMP\_NUM\_THREADS} to control the number of threads we use. As $p$ increases, the time taken for parallel scan decreases and scale as the threads increases when $p\leq 8$.
After $p\geq 64$, the time does not decrease because we only have 64 cores.

\begin{table}[tbhp]
	\label{tab:scan}
	\caption{The time for parallel scan using OpenMP with different threads number $p$. The vector length is $N=10^8.$}
		\centering
	\begin{tabular}{c|c|c}
		\hline 
Threads number $p$	& Sequential-scan [s]  &  Parallel-scan [s]\\ 
		\hline 
	1	& 2.214138 & 1.409567 \\ 
	2	& 2.559628& 1.313942 \\ 
	4	& 2.235220&  0.815714\\ 
	8 & 2.504774 & 0.442005\\
	16 & 2.228650 & 0.504438 \\
	32 & 2.295501 & 0.334572 \\
	64 & 2.223448 & 0.303463 \\
	128 & 2.208624 & 0.414760 \\
		\hline 
	\end{tabular} 
\end{table}


\section{OpenMP version of 2D Jacobi/Gauss-Seidel smoothing}
The machine I use is the same as in last section. In \cref{tab:L2D-100}, I compare the time for different methods when using different number of threads. At first (number of threads $\leq 8$), the time decreases as the scaling of the increase of threads, after 8 threads, the time increases. The times for these two methods are similar when using same number of threads.
\begin{table}[tbhp]
	\label{tab:L2D-100}
	\caption{The time comparison for Jacobi and Gauss-Seidel method using different number of threads. The mesh size $N=100$ and iteration number is 5000. The initial residual for both methods is 100, Jacobi ends with residual: 7.284635, Gauss-Seidel ends with residual: 0.917052.}
	\centering
	\begin{tabular}{c|c|c}
		\hline 
		Threads number & Jacobi [s]  &  Gauss-Seidel [s]\\ 
		\hline 
		1	& 1.666097 & 1.475124 \\ 
		2	& 0.905389& 0.767714 \\ 
		4	&  0.490199&  0.412829\\ 
		8 & 0.294514 & 0.254353\\
		16 & 0.234049 & 0.253860\\
		32 & 0.348937 &  0.455067 \\
		\hline 
	\end{tabular} 
\end{table}

In \cref{tab:L2D-N}, I compare the time and convergence for different $N$'s. The time for both methods are similar, while the Gauss-Seidel converges faster than Jacobi, but this difference decreases as $N$ increases.At the first few iterations, Jacobi converges more quickly and stably than the Gauss-Seidel,  but in the end, the Gauss-Seidel will achieve much faster convergence. As $N$ increase, we also observe the convergence factor decreases rapidly for both methods.

\begin{table}[tbhp]
	\label{tab:L2D-N}
	\caption{The comparison for Jacobi and Gauss-Seidel method using different $N=100$ and iteration number is 5000, the threads number is 8. We compare the times as well as the convergence speed of both methods. The convergence speed is characterized by the factor between the initial residual and the final residual, larger factor means faster convergence.}
	\centering
	\begin{tabular}{c|c c|r r}
		\hline 
		$N$ & Jacobi time [s]  &  Gauss-Seidel time [s] &  Jacobi factor &  Gauss-Seidel factor \\ 
		\hline 
		50 &  0.139663 & 0.108166 & 16045.910103 & 150285452.645879\\
		100 & 0.294316 & 0.266196 &  13.727524 & 109.045039 \\
		150 & 0.635592 & 0.494592 & 3.617021 &  7.547468\\
		200 &  1.035472 &   0.815142 & 2.259144 &  2.944490 \\
		\hline 
	\end{tabular} 
\end{table}





\end{document}